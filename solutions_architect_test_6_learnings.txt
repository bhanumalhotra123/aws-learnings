Test6

1.Other AWS services, such as S3, DynamoDB, and Lambda, can assume roles. This is because they are AWS principals. EC2 instances are not AWS principals, so they cannot assume roles directly. Instead, they must use instance profiles to assume roles.
An instance profile is a container for an IAM role. When you launch an EC2 instance with an instance profile assigned to it, the instance assumes the role and has the permissions that are defined in the role.
Using instance profiles to grant EC2 instances access to AWS resources has several benefits, including:
Security: Instance profiles can help to improve security by ensuring that only the permissions that are needed are granted to EC2 instances.
Here is an example of how to grant an EC2 instance access to S3 using an instance profile:
Create an IAM role with the permissions to access S3.
Create an instance profile and associate it with the IAM role.
Launch your EC2 instance with the instance profile assigned to it.

2.AWS Firewall Manager is a security management service which allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. As new applications are created, Firewall Manager makes it easy to bring new applications and resources into compliance by enforcing a common set of security rules. Now you have a single service to build firewall rules, create security policies, and enforce them in a consistent, hierarchical manner across your entire infrastructure.
  
Using AWS Firewall Manager, you can centrally configure AWS WAF rules, AWS Shield Advanced protection, Amazon Virtual Private Cloud (VPC) security groups, AWS Network Firewalls, and Amazon Route 53 Resolver DNS Firewall rules across accounts and resources in your organization. It does not support Network ACLs as of today.  

3. For higher levels of protection against attacks targeting your applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53 resources, you can subscribe to AWS Shield Advanced. In addition to the network and transport layer protections that come with Standard, AWS Shield Advanced provides additional detection and mitigation against large and sophisticated DDoS attacks, near real-time visibility into attacks, and integration with AWS WAF, a web application firewall. AWS Shield Advanced also gives you 24x7 access to the AWS DDoS Response Team (DRT) and protection against DDoS-related spikes in your Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53 charges.

4. EFS has a higher throughput than EBS. In addition, EBS can be attached to multiple EC2 instances when the underlying EBS type is io1/io2 and the instance is of Nitro type. The use-case does not provide any such details, so this option is ruled out.

5. storage limit for each item in a DynamoDB table is 400 KB

6. With Bursting Throughput mode, a file system's throughput scales as the amount of data stored in the standard storage class grows. File-based workloads are typically spiky, driving high levels of throughput for short periods of time, and low levels of throughput the rest of the time. To accommodate this, Amazon EFS is designed to burst to high throughput levels for periods of time. By default, AWS recommends that you run your application in the Bursting Throughput mode. But, if you're planning to migrate large amounts of data into your file system, consider switching to Provisioned Throughput mode.  

7.Customer Managed Key (CMK) and Customer Master Key (CMK) are synonymous 
Customer Master Key (CMK) is the key used to manage other keys, including DEKs.
Data Encryption Key (DEK) is the key used to encrypt and decrypt data.  

8.Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3):

CMK: Amazon S3 holds the CMK, which is used for managing encryption for the objects in your S3 bucket.
DEK: Amazon S3 generates a unique DEK for each object and holds it. It uses this DEK to encrypt and decrypt the object's data.

Server-Side Encryption with AWS Key Management Service (SSE-KMS):

CMK: AWS Key Management Service (KMS) holds the CMK. You can choose between AWS-managed CMKs or create your own Customer Managed CMK.
DEK: KMS generates and manages the DEKs. Each object is encrypted with a unique DEK, and KMS holds these DEKs. KMS uses the CMK to protect and manage the DEKs.

Server-Side Encryption with Customer-Provided Keys (SSE-C):

CMK: In SSE-C, you (the customer) provide and manage the CMK. This is typically an external key you supply when you upload an object to S3.
DEK: You also provide and manage the DEK. The DEK is used to encrypt and decrypt the object's data, and you are responsible for securely storing and managing it.  

Server-Side Encryption (SSE) occurs on the server side, with the cloud storage service managing encryption keys.
Client-Side Encryption occurs on the client side, with the client responsible for encryption, decryption, and key management.  

In order to enforce object encryption, create an S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption header. There are two possible values for the x-amz-server-side-encryption header: AES256, which tells S3 to use S3-managed keys, and aws:kms, which tells S3 to use AWS KMSâ€“managed keys.


9.In AWS, for example, Application Load Balancer (ALB) supports TLS termination (offloading), whereas Network Load Balancer (NLB) is typically used for TLS pass-through, where the decryption happens on the backend servers themselves.

10.You can transfer up to 100PB per Snowmobile

11.AWS recommends snowball only if you want to transfer greater than 10 TB of data between your on-premises data centers and Amazon S3.

There are three main types of EBS RAIDs:

RAID 0: RAID 0 stripes data across multiple volumes, but does not mirror it. This means that if one volume fails, all of the data on the RAID is lost. However, RAID 0 provides the best performance of all RAID types.
RAID 1: RAID 1 mirrors data across multiple volumes. This means that if one volume fails, the data is still available on the other volumes. However, RAID 1 only provides half of the performance of a single volume.
RAID 10: RAID 10 is a combination of RAID 0 and RAID 1. It stripes data across multiple volumes and then mirrors the striped data across additional volumes. This provides the best performance and reliability of all RAID types.

12. An encryption context is a set of key-value pairs that contain additional contextual information about the data. When an encryption context is specified for an encryption operation, Amazon S3 must specify the same encryption context for the decryption operation. The encryption context offers another level of security for the encryption key. However, it is not useful for generating unique keys.

 If you only need to ensure that each object is encrypted with a unique key, then you can use SSE-S3. There is no need to manually add encryption context to each upload.

13.You can create up to five read replicas from one DB instance. For replication to operate effectively, each read replica should have the same amount of compute and storage resources as the source DB instance.  

14.Running a DB instance as a Multi-AZ deployment can further reduce the impact of a maintenance event because Amazon RDS applies operating system updates by following these steps:
Perform maintenance on the standby.
Promote the standby to primary.
Perform maintenance on the old primary, which becomes the new standby.

 In Amazon RDS, you must manually promote a read replica to primary in the event of a failover. However, in a Multi-AZ deployment, Amazon RDS will automatically fail over to the standby DB instance if the primary DB instance fails.  

15. You are correct that spread placement groups keep all instances separately, but they do not guarantee that the instances will be placed on separate underlying hardware.
This is because Amazon EC2 may not have enough physical servers in a single Availability Zone to support all of the instances in a spread placement group. In this case, Amazon EC2 will place some of the instances on the same underlying hardware.
Partition placement groups, on the other hand, guarantee that the instances will be placed on separate underlying hardware. This is because partition placement groups divide the placement group into logical partitions and ensure that no two partitions share the same underlying hardware.

 you can create multiple partition placement groups in the same AZ, and you can distribute your instances across the multiple partition placement groups.

16.The maximum timeout value for any AWS Lambda function is 15 minutes. When the specified timeout is reached, AWS Lambda terminates the execution of your Lambda function. 

17. Kinesis Data Streams (KDS) and Kinesis Data Analytics are not suitable for long-term storage of data, unlike databases or data warehouses. Additionally, it mentions that in Kinesis Data Streams, you need to manually allocate shards for scaling the data ingestion process, which is an important consideration for users of this service. 

18. If you create an AMI from an instance, the data on its instance store volumes isn't preserved and isn't present on the instance store volumes of the instances that you launch from the AMI.  

19. ELB cannot distribute incoming traffic for targets deployed in different regions  

20.  By default, the DeleteOnTermination attribute is set to True for the root volume and is set to False for all other volume types.
If the instance is already running, you can set DeleteOnTermination to False using the command line.

21.A byte-range request is a perfect way to get the beginning of a file and ensuring we remain efficient during our scan of our S3 bucket.
S3 Select is a new Amazon S3 capability designed to pull out only the data you need from an object, which can dramatically improve the performance and reduce the cost of applications that need to access data in S3. 

22.Spot instances are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification
Spot blocks allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted

A Spot Fleet is a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity
 A Spot Fleet allows you to automatically request and manage multiple Spot instances that provide the lowest price per unit of capacity for your cluster or application, like a batch processing job, a Hadoop workflow, or an HPC grid computing job.

23.To securely serve this private content by using CloudFront, you can do the following:
Require that your users access your private content by using special CloudFront signed URLs or signed cookies.
A signed URL includes additional information, for example, expiration date and time, that gives you more control over access to your content. 

CloudFront signed cookies allow you to control who can access your content when you don't want to change your current URLs or when you want to provide access to multiple restricted files, for example, all of the files in the subscribers' area of a website.

24. You can update your configuration to ensure that your user data scripts and cloud-init directives run every time you restart your instance. By default, the scripts are run, only once during the boot process while first launching the instance.
You can pass two types of user data to Amazon EC2: shell scripts and cloud-init directives.
By default, user data scripts and cloud-init directives run only during the boot cycle when you first launch an instance. Hence, no extra configuration is needed, apart from including the custom scripts in user data scripts.

cloud-init is a specialized initialization system used in cloud instances to handle configuration tasks, while user data is a more general concept used to pass custom data or scripts to instances. 

25.Redshift's internal storage does not have "tiers" of storage classes like Amazon S3
Moving the data to S3 glacier will prevent us from being able to query it. Therefore, we should migrate the data to S3 Standard IA and use Athena to analyze the cold data.

26.Synchronous programming is a sequential approach, where code is executed one line at a time, in the order it is written. The program will wait for each line of code to finish executing before moving on to the next line.

Asynchronous programming is a non-sequential approach, where code can be executed in parallel. The program does not have to wait for each line of code to finish executing before moving on to the next line.

27.As the Availability Zones got unbalanced, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application  

Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance

Here is a more detailed explanation of the rebalancing process:

Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance.
Amazon EC2 Auto Scaling launches a new instance to replace the terminated instance.
Amazon EC2 Auto Scaling waits for the new instance to become healthy.
Amazon EC2 Auto Scaling terminates the unhealthy instance.

28. long polling or short polling cannot impact the priority of processing for the two queues

AWS recommends using separate queues to provide prioritization of work. Therefore, for the given use case, you need to create an SQS standard queue for processing pro users' photos and another SQS standard queue for processing lite users' photos. Then you can configure EC2 instances to prioritize polling for the pro queue over the lite queue.   

29.When you upgrade an RDS DB instance to a new database engine version, Amazon RDS upgrades the following components:

The database engine software
The database schema
The database data

The upgrade process involves the following steps:

Amazon RDS takes a snapshot of the database instance. This snapshot is used to restore the database instance if the upgrade fails.
Amazon RDS upgrades the database engine software.
Amazon RDS upgrades the database schema.
Amazon RDS upgrades the database data.
Amazon RDS verifies that the upgrade was successful.

If you are using a Multi-AZ deployment, both the primary and standby DB instances are upgraded at the same time. This means that your database will be unavailable during the entire upgrade process.  

30. we have to pay for inter-Region data replication charges for the Read Replica, whereas the replication of data within a single Region is free.
Enabling Multi-AZ helps make our database highly-available, but the standby database is not accessible and cannot be used for reads or write. It's just a database that will become primary when the other database encounters a failure.

31.AWS recommends that you should use AWS DataSync to migrate existing data to Amazon S3, and subsequently use the File Gateway configuration of AWS Storage Gateway to retain access to the migrated data and for ongoing updates from your on-premises file-based applications.

The Volume Gateway provides block storage to your on-premises applications using iSCSI connectivity. Data on the volumes is stored in Amazon S3 and you can take point in time copies of volumes that are stored in AWS as Amazon EBS snapshots. Volume Gateway is for block storage and not for file storage 

32.AWS Managed Microsoft AD is the best choice for organizations that need a fully managed Microsoft Active Directory service in the cloud. It supports all Active Directory features, including trust relationships, Group Policy, and Kerberos SSO. This makes it easy to migrate your on-premises Active Directory to AWS or to create a new Active Directory environment in the cloud.

AD Connector is the best choice for organizations that want to connect their on-premises Active Directory to AWS. This allows you to use your existing Active Directory credentials to access AWS services and applications. You can also use AD Connector to join EC2 instances to your on-premises Active Directory domain.

Simple AD is the best choice for organizations that need a low-scale, low-cost directory with basic Active Directory compatibility. It supports basic Active Directory features such as user accounts, group memberships, and LDAP compatibility. Simple AD is a good choice for small businesses and organizations that are developing new applications.

33. AWS Data Pipeline provides a managed orchestration service that gives you greater flexibility in terms of the execution environment, access and control over the compute resources that run your code, as well as the code itself that does data processing. AWS Data Pipeline launches compute resources in your account allowing you direct access to the Amazon EC2 instances or Amazon EMR clusters. As this option provides access to the underlying EC2 instances so it's not a serverless solution. Therefore this option is incorrect for the given use case.

34.Lambda would be the perfect fit if our script could run in less than 15 minutes, as this is the maximum timeout for Lambda.

35. the enableDnsHostnames and enableDnsSupport attributes must be set to true in the VPC that you want to associate with the private hosted zone in the other VPC. This is because DNS resolution is required for private hosted zones.
Note: You can only associate VPCs within the same region with a private hosted zone.

36.Multi-attach EBS volumes are supported only for Nitro EC2 instances which are Linux-based. Multi-Attach is supported exclusively on Provisioned IOPS SSD volumes.
 EFS is not compatible with the Windows platform

37.Here is an example of how you could use EMR to create the daily big data analysis job:

Create an EMR cluster with the appropriate Spark version and configuration.
Write a Spark job to read the data from S3, analyze it, and generate the customized reports.
Submit the Spark job to the EMR cluster.
Monitor the EMR cluster and the Spark job to ensure that it is running successfully.
Once the job is complete, the customized reports will be saved to S3.
You can use the Amazon EMR console, the AWS CLI, or the AWS SDK to create and manage EMR clusters and to submit Spark jobs.

Here is an example of how you could use Glue to create the daily big data analysis job:

Create a Glue job to read the data from S3, analyze it, and generate the customized reports.
Schedule the Glue job to run daily.
Glue will run the job on your behalf and save the customized reports to S3.

38.between the ALB and the Elastic IP. If we use an ALB, things will still work, but we will have to pay for the provisioned ALB which sends traffic to only one EC2 instance. Instead, to minimize costs, we must use an Elastic IP.

39.Temporary queues: Temporary queues are automatically deleted after a certain period of inactivity. This makes them ideal for short-lived tasks, such as processing a batch of messages from a queue and then deleting the queue.
Dead-letter queues (DLQs): DLQs are used to store messages that fail to be processed by consumers. This can happen for a variety of reasons, such as if the consumer goes down or if the message is malformed. DLQs can be used to retry processing failed messages or to investigate the cause of the failure.
Scheduled queues: Scheduled queues allow you to schedule messages to be delivered at a specific time in the future. This can be useful for tasks such as sending out daily email digests or processing reports at the end of the day.

40.As a solutions architect, the best architecture for this requirement is to use AWS Direct Connect with a dual-redundant configuration. This means that the company will have two AWS Direct Connect connections, each of which is terminating on a different device in a different location. This will provide redundancy in the event of a failure of one of the connections or devices.

41.To set up a rate-based rule in AWS WAF, you can use the AWS WAF console, the AWS CLI, or the AWS WAF API. The rule will specify the following:

The type of request to block (e.g., GET, POST, etc.)
The path of the resource to protect (e.g., /index.html)
The rate at which requests are blocked (e.g., 100 requests per second)
The duration for which requests are blocked (e.g., 1 minute)

42. EventBridge events cannot invoke applications on EC2 instances
Amazon S3 supports the following destinations where it can publish events:
Amazon Simple Notification Service (Amazon SNS) topic
Amazon Simple Queue Service (Amazon SQS) queue
AWS Lambda
