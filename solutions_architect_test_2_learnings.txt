Test2

1.User Data is generally used to perform common automated configuration tasks and even run scripts after the instance starts. When you launch an instance in Amazon EC2, you can pass two types of user data - shell scripts and cloud-init directives. You can also pass this data into the launch wizard as plain text or as a file.

Scripts entered as user data are executed as the root user, hence do not need the sudo command in the script. Any files you create will be owned by root; if you need non-root users to have file access, you should modify the permissions accordingly in the script.

By default, user data scripts and cloud-init directives run only during the boot cycle when you first launch an instance. You can update your configuration to ensure that your user data scripts and cloud-init directives run every time you restart your instance.


2.You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, Organizations SCPs, ACLs, and session policies.

Identity-Based Policies: Policies attached to IAM users, groups, or roles to define what actions they can perform.

Resource-Based Policies: Policies attached to AWS resources, such as S3 buckets or Lambda functions, to control access from other AWS accounts or entities.

Permissions Boundaries: IAM policy that sets the maximum permissions a user or role can have, allowing more fine-grained control.

Organizations SCPs (Service Control Policies): Policies applied at the organizational level to control permissions for member accounts within AWS Organizations.

ACLs (Access Control Lists): Legacy method for managing permissions on S3 buckets and objects, providing granular control over who can access the resources.

Session Policies: Policies associated with temporary security credentials assumed via AWS STS (Security Token Service) to further restrict permissions for a session or role.

3.A launch template is similar to a launch configuration, in that it specifies instance configuration information such as the ID of the Amazon Machine Image (AMI), the instance type, a key pair, security groups, and the other parameters that you use to launch EC2 instances. Also, defining a launch template instead of a launch configuration allows you to have multiple versions of a template.

With launch templates, you can provision capacity across multiple instance types using both On-Demand Instances and Spot Instances to achieve the desired scale, performance, and cost.

A launch configuration is an instance configuration template that an Auto Scaling group uses to launch EC2 instances. When you create a launch configuration, you specify information for the instances such as the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping.

You cannot use a launch configuration to provision capacity across multiple instance types using both On-Demand Instances and Spot Instances. 

AWSTemplateFormatVersion: '2010-09-09'
Resources:
  MyAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      LaunchTemplate:
        LaunchTemplateName: MyLaunchTemplate
      MinSize: 5 # Base capacity
      MaxSize: 20 # Maximum desired capacity
      DesiredCapacity: 10
      MixedInstancesPolicy:
        InstancesDistribution:
          OnDemandPercentageAboveBaseCapacity: 30
          SpotAllocationStrategy: lowest-price # or other strategy

With this configuration, when your Auto Scaling group is set to a desired capacity of 10 instances, here's what happens:

The base capacity is 5 On-Demand Instances (MinSize).
The additional instances above the base capacity are calculated as 30% of 5, which is 1.5 (rounded up to 2).
So, you have 5 base On-Demand Instances and 2 more On-Demand Instances above the base capacity, totaling 7 On-Demand Instances.
The remaining 3 instances (10 total - 7 On-Demand) are Spot Instances.

4.Application Load Balancer can be used to securely authenticate users for accessing your applications. This enables you to offload the work of authenticating users to your load balancer so that your applications can focus on their business logic. You can use Cognito User Pools to authenticate users through well-known social IdPs, such as Amazon, Facebook, or Google, through the user pools supported by Amazon Cognito or through corporate identities, using SAML, LDAP, or Microsoft AD, through the user pools supported by Amazon Cognito. You configure user authentication by creating an authenticate action for one or more listener rules.

There is no such thing as using Cognito Authentication via Cognito Identity Pools for managing user authentication for the application. Application-specific user authentication can be provided via Cognito User Pools. Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token.

You cannot directly integrate Cognito User Pools with CloudFront distribution as you have to create a separate Lambda@Edge function to accomplish the authentication via Cognito User Pools. This involves additional development effort, so this option is not the best fit for the given use-case.

5.FSx for Lustre is a file system better suited for distributed computing for HPC (high-performance computing) and is very expensive

6.AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. SSL/TLS certificates are used to secure network communications and establish the identity of websites over the Internet as well as resources on private networks.

AWS Config provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time.

AWS Config provides AWS-managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. You can leverage an AWS Config managed rule to check if any ACM certificates in your account are marked for expiration within the specified number of days. Certificates provided by ACM are automatically renewed. ACM does not automatically renew the certificates that you import. The rule is NON_COMPLIANT if your certificates are about to expire.

You can configure AWS Config to stream configuration changes and notifications to an Amazon SNS topic. For example, when a resource is updated, you can get a notification sent to your email, so that you can view the changes. You can also be no



Certificate Request: When you request an SSL/TLS certificate, you specify the domain(s) you want to secure, such as "example.com" or "www.example.com."

Validation Methods: To prove that you have control over the domain, the CA uses various validation methods. One of the common methods is DNS validation.

DNS Validation: For DNS validation, the CA provides you with a unique value (a cryptographic token) that you need to add to your domain's DNS configuration as a DNS record. This record is usually a CNAME (Canonical Name) or TXT (Text) record with a specific name and value.

CA Checks: The CA then checks the DNS records to confirm that the provided value matches what is in the DNS configuration for the domain. If the validation is successful, it means that you have control over the domain, and the certificate is issued.

Certificate Installation: Once the certificate is issued, you install it on your web server or load balancer to enable secure HTTPS connections.

Certificate Renewal: SSL/TLS certificates have a limited validity period, typically ranging from a few months to a few years. To maintain secure connections, certificates need to be renewed before they expire.

Renewal Validation: During certificate renewal, the CA may repeat the validation process, which includes checking the DNS records to ensure that the domain owner still has control over it.

The DNS records used for certificate validation are critical during both the initial certificate issuance and subsequent renewals. If these DNS records are missing, modified, or incorrect, the CA cannot validate the domain's ownership, and the certificate renewal may fail. This could result in the loss of HTTPS connectivity and pose security risks.

Therefore, it's crucial to maintain and regularly monitor your DNS records, especially those used for SSL/TLS certificate validation, to ensure that certificates are renewed successfully and web services remain secure. Certificate management services like ACM help automate and streamline this process.





Certificate Request: When you request an SSL/TLS certificate, you specify the domain(s) you want to secure, such as "example.com" or "www.example.com."

Validation Methods: To prove that you have control over the domain, the CA uses various validation methods. One of the common methods is DNS validation.

DNS Validation: For DNS validation, the CA provides you with a unique value (a cryptographic token) that you need to add to your domain's DNS configuration as a DNS record. This record is usually a CNAME (Canonical Name) or TXT (Text) record with a specific name and value.

CA Checks: The CA then checks the DNS records to confirm that the provided value matches what is in the DNS configuration for the domain. If the validation is successful, it means that you have control over the domain, and the certificate is issued.

Certificate Installation: Once the certificate is issued, you install it on your web server or load balancer to enable secure HTTPS connections.

Certificate Renewal: SSL/TLS certificates have a limited validity period, typically ranging from a few months to a few years. To maintain secure connections, certificates need to be renewed before they expire.

Renewal Validation: During certificate renewal, the CA may repeat the validation process, which includes checking the DNS records to ensure that the domain owner still has control over it.

The DNS records used for certificate validation are critical during both the initial certificate issuance and subsequent renewals. If these DNS records are missing, modified, or incorrect, the CA cannot validate the domain's ownership, and the certificate renewal may fail. This could result in the loss of HTTPS connectivity and pose security risks.

Therefore, it's crucial to maintain and regularly monitor your DNS records, especially those used for SSL/TLS certificate validation, to ensure that certificates are renewed successfully and web services remain secure. Certificate management services like ACM help automate and streamline this process.





Certificate Request: When you request an SSL/TLS certificate, you specify the domain(s) you want to secure, such as "example.com" or "www.example.com."

Validation Methods: To prove that you have control over the domain, the CA uses various validation methods. One of the common methods is DNS validation.

DNS Validation: For DNS validation, the CA provides you with a unique value (a cryptographic token) that you need to add to your domain's DNS configuration as a DNS record. This record is usually a CNAME (Canonical Name) or TXT (Text) record with a specific name and value.

CA Checks: The CA then checks the DNS records to confirm that the provided value matches what is in the DNS configuration for the domain. If the validation is successful, it means that you have control over the domain, and the certificate is issued.

Certificate Installation: Once the certificate is issued, you install it on your web server or load balancer to enable secure HTTPS connections.

Certificate Renewal: SSL/TLS certificates have a limited validity period, typically ranging from a few months to a few years. To maintain secure connections, certificates need to be renewed before they expire.

Renewal Validation: During certificate renewal, the CA may repeat the validation process, which includes checking the DNS records to ensure that the domain owner still has control over it.

The DNS records used for certificate validation are critical during both the initial certificate issuance and subsequent renewals. If these DNS records are missing, modified, or incorrect, the CA cannot validate the domain's ownership, and the certificate renewal may fail. This could result in the loss of HTTPS connectivity and pose security risks.

Therefore, it's crucial to maintain and regularly monitor your DNS records, especially those used for SSL/TLS certificate validation, to ensure that certificates are renewed successfully and web services remain secure. Certificate management services like ACM help automate and streamline this process.





The relationship between SSL/TLS certificates and DNS records is closely tied to the process of certificate issuance and validation. When you secure a domain or a subdomain with an SSL/TLS certificate, the Certificate Authority (CA), which could be a third-party CA or a service like AWS Certificate Manager (ACM), needs to confirm that you have control over the domain for which you're requesting the certificate. This verification is crucial to prevent fraudulent certificates from being issued.

Here's how the relationship works:

Certificate Request: When you request an SSL/TLS certificate, you specify the domain(s) you want to secure, such as "example.com" or "www.example.com."

Validation Methods: To prove that you have control over the domain, the CA uses various validation methods. One of the common methods is DNS validation.

DNS Validation: For DNS validation, the CA provides you with a unique value (a cryptographic token) that you need to add to your domain's DNS configuration as a DNS record. This record is usually a CNAME (Canonical Name) or TXT (Text) record with a specific name and value.

CA Checks: The CA then checks the DNS records to confirm that the provided value matches what is in the DNS configuration for the domain. If the validation is successful, it means that you have control over the domain, and the certificate is issued.

Certificate Installation: Once the certificate is issued, you install it on your web server or load balancer to enable secure HTTPS connections.

Certificate Renewal: SSL/TLS certificates have a limited validity period, typically ranging from a few months to a few years. To maintain secure connections, certificates need to be renewed before they expire.

Renewal Validation: During certificate renewal, the CA may repeat the validation process, which includes checking the DNS records to ensure that the domain owner still has control over it.

The DNS records used for certificate validation are critical during both the initial certificate issuance and subsequent renewals. If these DNS records are missing, modified, or incorrect, the CA cannot validate the domain's ownership, and the certificate renewal may fail. This could result in the loss of HTTPS connectivity and pose security risks.

Therefore, it's crucial to maintain and regularly monitor your DNS records, especially those used for SSL/TLS certificate validation, to ensure that certificates are renewed successfully and web services remain secure. Certificate management services like ACM help automate and streamline this process.



AWS Certificate Manager (ACM) does not attempt to renew third-party certificates that are imported. Also, an administrator needs to reconfigure missing DNS records for certificates that use DNS validation if the record was removed for any reason after the certificate was issued. Metrics and events provide you visibility into such certificates that require intervention to continue the renewal process. Amazon CloudWatch metrics and Amazon EventBridge events are enabled for all certificates that are managed by ACM. Users can monitor days to expiry as a metric for ACM certificates through Amazon CloudWatch. An Amazon EventBridge expiry event is published for any certificate that is at least 45 days away from expiry by default. Users can build alarms to monitor certificates based on days to expiry and also trigger custom actions such as calling a Lambda function or paging an administrator.

It is certainly possible to use the days to expiry CloudWatch metric to build a CloudWatch alarm to monitor the imported ACM certificates. The alarm will, in turn, trigger a notification to the security team. But this option needs more configuration effort than directly using the AWS Config managed rule that is available off-the-shelf.

Any SSL/TLS certificates created via ACM do not need any monitoring/intervention for expiration. ACM automatically renews such certificates.





7.Use Amazon S3 Bucket Policies

Bucket policies in Amazon S3 can be used to add or deny permissions across some or all of the objects within a single bucket. Policies can be attached to users, groups, or Amazon S3 buckets, enabling centralized management of permissions. With bucket policies, you can grant users within your AWS Account or other AWS Accounts access to your Amazon S3 resources.

You can further restrict access to specific resources based on certain conditions. For example, you can restrict access based on request time (Date Condition), whether the request was sent using SSL (Boolean Conditions), a requester’s IP address (IP Address Condition), or based on the requester's client application (String Conditions). To identify these conditions, you use policy keys.

Use Identity and Access Management (IAM) policies - AWS IAM enables organizations with many employees to create and manage multiple users under a single AWS account. IAM policies are attached to the users, enabling centralized control of permissions for users under your AWS Account to access buckets or objects. With IAM policies, you can only grant users within your own AWS account permission to access your Amazon S3 resources. So, this is not the right choice for the current requirement.

Use Access Control Lists (ACLs) - Within Amazon S3, you can use ACLs to give read or write access on buckets or objects to groups of users. With ACLs, you can only grant other AWS accounts (not specific users) access to your Amazon S3 resources. So, this is not the right choice for the current requirement.



8.If your workload is unpredictable, you can enable storage autoscaling for an Amazon RDS DB instance. With storage autoscaling enabled, when Amazon RDS detects that you are running out of free database space it automatically scales up your storage. Amazon RDS starts a storage modification for an autoscaling-enabled DB instance when these factors apply:
Free available space is less than 10 percent of the allocated storage.
The low-storage condition lasts at least five minutes.
At least six hours have passed since the last storage modification.
The maximum storage threshold is the limit that you set for autoscaling the DB instance. You can't set the maximum storage threshold for autoscaling-enabled instances to a value greater than the maximum allocated storage.

Although Aurora offers automatic storage scaling, this option is ruled out since it involves significant systems administration effort to migrate from RDS MySQL to Aurora. It is much easier to just enable storage auto-scaling for RDS MySQL.


9. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. With Amazon Kinesis Data Streams, you can scale up to a sufficient number of shards (note, however, that you'll need to provision enough shards ahead of time). As it requires manual administration of shards, it's not the correct choice for the given use-case.

AWS Lambda lets you run code without provisioning or managing servers. It cannot be used for production-grade serverless log analytics.

10.Amazon S3 is an object storage service. Amazon S3 makes data available through an Internet API that can be accessed anywhere. It is not a file storage service, as is needed in the use case.

11.Blue/green deployment is a technique for releasing applications by shifting traffic between two identical environments running different versions of the application: "Blue" is the currently running version and "green" the new version. This type of deployment allows you to test features in the green environment without impacting the currently running version of your application. When you’re satisfied that the green version is working properly, you can gradually reroute the traffic from the old blue environment to the new green environment. Blue/green deployments can mitigate common risks associated with deploying software, such as downtime and rollback capability.


AWS Global Accelerator does not provide caching capabilities. It is primarily a service for routing and directing traffic to AWS services and endpoints in a highly available and performant manner, but it does not cache or store content. Content caching and delivery are typically handled by services like Amazon CloudFront, which is a CDN (Content Delivery Network) service.


 AWS Global Accelerator is a powerful solution for global traffic distribution and can help with minimizing the impact of DNS caching.

12.With AWS Lambda, you can run code without provisioning or managing servers. You can't host a website on Lambda. Also, you can't have CloudFront in front of Lambda. So this option is incorrect.

13.AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. You cannot transition the refined zone data into Glacier Deep Archive because it is used by the business analysts for ad-hoc querying. Therefore, the best optimization is to have the refined zone data stored in a compressed format via the Glue job. The compressed data would reduce the storage cost incurred on the data in the refined zone.(on the base of question)

14.Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. 

15.There are data transfer charges for replicating data across AWS Regions

RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.

A read replica is billed as a standard DB Instance and at the same rates. You are not charged for the data transfer incurred in replicating data between your source DB instance and read replica within the same AWS Region.

16.Max I/O performance mode is used to scale to higher levels of aggregate throughput and operations per second. This scaling is done with a tradeoff of slightly higher latencies for file metadata operations. Highly parallelized applications and workloads, such as big data analysis, media processing, and genomic analysis, can benefit from this mode.

17.You can use the aws:RequestedRegion key to compare the AWS Region that was called in the request with the Region that you specify in the policy. You can use this global condition key to control which Regions can be requested.

aws:RequestedRegion represents the target of the API call. 

18.CloudFormation takes time to provision the resources and hence is not the right solution when LEAST amount of downtime is mandated for the given use case.

19.In a multi-master cluster, all DB instances can perform write operations. There isn't any failover when a writer DB instance becomes unavailable, because another writer DB instance is immediately available to take over the work of the failed instance. AWS refers to this type of availability as continuous availability, to distinguish it from the high availability (with brief downtime during failover) offered by a single-master cluster. For applications where you can't afford even brief downtime for database write operations, a multi-master cluster can help to avoid an outage when a writer instance becomes unavailable. The multi-master cluster doesn't use the failover mechanism, because it doesn't need to promote another DB instance to have read/write capability.

20.The Distributed File System Replication (DFSR) service is a new multi-master replication engine that is used to keep folders synchronized on multiple servers. Amazon FSx supports the use of Microsoft’s Distributed File System (DFS) to organize shares into a single folder structure up to hundreds of PB in size.

FSx for Windows is a perfect distributed file system, with replication capability, and can be mounted on Windows.

21.AWS Resource Access Manager (RAM) is a service that enables you to easily and securely share AWS resources with any AWS account or within your AWS Organization. You can share AWS Transit Gateways, Subnets, AWS License Manager configurations, and Amazon Route 53 Resolver rules resources with RAM. RAM eliminates the need to create duplicate resources in multiple accounts, reducing the operational overhead of managing those resources in every single account you own. You can create resources centrally in a multi-account environment, and use RAM to share those resources across accounts in three simple steps: create a Resource Share, specify resources, and specify accounts. RAM is available to you at no additional charge.

The correct solution is to share the subnet(s) within a VPC using RAM. This will allow all EC2 instances to be deployed in the same VPC (although from different accounts) and easily communicate with one another.

 AWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway. A Transit Gateway will work but will be an expensive solution. Here we want to minimize cost.

22. summary, the primary difference is that with SSE-C, you bring your own encryption keys, while with SSE-KMS using customer managed keys, you leverage AWS KMS to create and manage encryption keys, but you have more control over the management of these keys. 

23.As EBS volumes are attached locally to the EC2 instances, therefore the uploaded videos are tied to specific EC2 instances. Every time the user logs in, they are directed to a different instance and therefore their videos get dispersed across multiple EBS volumes. The correct solution is to use either S3 or EFS to store the user videos.

24.Restoring the dev database via mysqldump would still result in a significant load on the primary DB

The standby is there just for handling failover in a Multi-AZ deployment. You cannot access the standby instance and use it as a dev database.

Aurora backs up your cluster volume automatically and retains restore data for the length of the backup retention period. Aurora backups are continuous and incremental so you can quickly restore to any point within the backup retention period. No performance impact or interruption of database service occurs as backup data is being written.

Automated backups occur daily during the preferred backup window. If the backup requires more time than allotted to the backup window, the backup continues after the window ends, until it finishes. The backup window can't overlap with the weekly maintenance window for the DB cluster. Aurora backups are continuous and incremental, but the backup window is used to create a daily system backup that is preserved within the backup retention period. The latest restorable time for a DB cluster is the most recent point at which you can restore your DB cluster, typically within 5 minutes of the current time.

For the given use case, you can create the dev database by restoring from the automated backups of Amazon Aurora.

25.Per the default termination policy, the first priority is given to any allocation strategy for On-Demand vs Spot instances. As no such information has been provided for the given use-case, so this criterion can be ignored. The next priority is to consider any instance with the oldest launch template unless there is an instance that uses a launch configuration. So this rules out Instance A. Next, you need to consider any instance which has the oldest launch configuration. This implies Instance B will be selected for termination and Instance C will also be ruled out as it has the newest launch configuration. Instance D, which is closest to the next billing hour, is not selected as this criterion is last in the order of priority.

26.Security groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic.

To enable the connection to a service running on an instance, the associated network ACL must allow both inbound traffic on the port that the service is listening on as well as allow outbound traffic from ephemeral ports. When a client connects to a server, a random port from the ephemeral port range (1024-65535) becomes the client's source port.

The designated ephemeral port then becomes the destination port for return traffic from the service, so outbound traffic from the ephemeral port must be allowed in the network ACL.

By default, network ACLs allow all inbound and outbound traffic. If your network ACL is more restrictive, then you need to explicitly allow traffic from the ephemeral port range.

If you accept traffic from the internet, then you also must establish a route through an internet gateway. If you accept traffic over VPN or AWS Direct Connect, then you must establish a route through a virtual private gateway.

27.AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data between on-premises storage systems and AWS Storage services, as well as between AWS Storage services.

You can use AWS DataSync to migrate data located on-premises, at the edge, or in other clouds to Amazon S3, Amazon EFS, Amazon FSx for Windows File Server, Amazon FSx for Lustre, Amazon FSx for OpenZFS, and Amazon FSx for NetApp ONTAP.

To establish a private connection between your virtual private cloud (VPC) and the Amazon EFS API, you can create an interface VPC endpoint. You can also access the interface VPC endpoint from on-premises environments or other VPCs using AWS VPN, AWS Direct Connect, or VPC peering.

AWS Direct Connect provides three types of virtual interfaces: public, private, and transit.
For the given use case, you can send data over the Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF.

Using task scheduling in AWS DataSync, you can periodically execute a transfer task from your source storage system to the destination. You can use the DataSync scheduled task to send the video files to the EFS file system every 24 hours.

28. Amazon EC2 Auto Scaling doesn't terminate an instance that came into service based on EC2 status checks and ELB health checks until the health check grace period expires.

 Amazon EC2 Auto Scaling does not immediately terminate instances with an Impaired status. Instead, Amazon EC2 Auto Scaling waits a few minutes for the instance to recover. Amazon EC2 Auto Scaling might also delay or not terminate instances that fail to report data for status checks. This usually happens when there is insufficient data for the status check metrics in Amazon CloudWatch.

By default, Amazon EC2 Auto Scaling doesn't use the results of ELB health checks to determine an instance's health status when the group's health check configuration is set to EC2. As a result, Amazon EC2 Auto Scaling doesn't terminate instances that fail ELB health checks. If an instance's status is OutofService on the ELB console, but the instance's status is Healthy on the Amazon EC2 Auto Scaling console, confirm that the health check type is set to ELB.



29.When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a root key that it regularly rotates. However, with SSE-S3, you cannot log the usage of the encryption key for auditing purposes. 

Server-side encryption with customer-provided keys (SSE-C) with automatic key rotation - It is possible to automatically rotate the customer-provided keys but you will need to develop the underlying solution to automate the key rotation. 

cryptographic keys that are used to protect your data. AWS KMS keys (KMS keys are also known as customer master key (CMK)) are the primary resource in AWS KMS. You can use a KMS key to encrypt, decrypt, and re-encrypt data. An AWS KMS key is a logical representation of a cryptographic key. A KMS key contains metadata, such as the key ID, key spec, key usage, creation date, description, and key state. Most importantly, it contains a reference to the key material that is used when you run cryptographic operations with the KMS key.

When you enable automatic key rotation for a KMS key, AWS KMS generates new cryptographic material for the KMS key every year.


30.AWS Transit Gateway is a service that enables customers to connect their Amazon Virtual Private Clouds (VPCs) and their on-premises networks to a single gateway. With AWS Transit Gateway, you only have to create and manage a single connection from the central gateway into each Amazon VPC, on-premises data center, or remote office across your network. Transit Gateway acts as a hub that controls how traffic is routed among all the connected networks which act like spokes. 

 AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network. Private Link is utilized to create a private connection between an application that is fronted by an NLB in an account, and an Elastic Network Interface (ENI) in another account, without the need of VPC peering, and allowing the connections between the two to remain within the AWS network.

31.DNS hostnames and DNS resolution are required settings for private hosted zones. DNS queries for private hosted zones can be resolved by the Amazon-provided VPC DNS server only. As a result, these options must be enabled for your private hosted zone to work.

DNS hostnames: For non-default virtual private clouds that aren't created using the Amazon VPC wizard, this option is disabled by default. If you create a private hosted zone for a domain and create records in the zone without enabling DNS hostnames, private hosted zones aren't enabled. To use a private hosted zone, this option must be enabled.

DNS resolution: Private hosted zones accept DNS queries only from a VPC DNS server. The IP address of the VPC DNS server is the reserved IP address at the base of the VPC IPv4 network range plus two. Enabling DNS resolution allows you to use the VPC DNS server as a Resolver for performing DNS resolution. Keep this option disabled if you're using a custom DNS server in the DHCP Options set, and you're not using a private hosted zone.


32.S3 batch replication can certainly be used to replicate the existing data in the current bucket in us-east-1 region into another bucket in us-west-1 region.

However, you cannot convert an existing single-Region key to a multi-Region key. This design ensures that all data protected with existing single-Region keys maintain the same data residency and data sovereignty properties. So this option is incorrect.

33. By default, an S3 object is owned by the AWS account that uploaded it. This is true even when the bucket is owned by another account. Because the Amazon Redshift data files from the UNLOAD command were put into your bucket by another account, you (the bucket owner) don't have default permission to access those files.

To get access to the data files, an AWS Identity and Access Management (IAM) role with cross-account permissions must run the UNLOAD command again. Follow these steps to set up the Amazon Redshift cluster with cross-account permissions to the bucket:

From the account of the S3 bucket, create an IAM role (Bucket Role) with permissions to the bucket.

From the account of the Amazon Redshift cluster, create another IAM role (Cluster Role) with permissions to assume the Bucket Role.

Update the Bucket Role to grant bucket access and create a trust relationship with the Cluster Role.

From the Amazon Redshift cluster, run the UNLOAD command using the Cluster Role and Bucket Role.

This solution doesn't apply to Amazon Redshift clusters or S3 buckets that use server-side encryption with AWS Key Management Service (AWS KMS).

34.Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups.

You can encrypt your Amazon RDS DB instances and snapshots at rest by enabling the encryption option for your Amazon RDS DB instances. Data that is encrypted at rest includes the underlying storage for DB instances, its automated backups, read replicas, and snapshots.

You can only enable encryption for an Amazon RDS DB instance when you create it, not after the DB instance is created. However, because you can encrypt a copy of an unencrypted DB snapshot, you can effectively add encryption to an unencrypted DB instance. That is, you can create a snapshot of your DB instance, and then create an encrypted copy of that snapshot. So this is the correct option.


35.Service control policies (SCPs) are one type of policy that you can use to manage your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization’s access control guidelines. SCPs are available only in an organization that has all features enabled. SCPs aren't available if your organization has enabled only the consolidated billing features. Attaching an SCP to an AWS Organizations entity (root, OU, or account) defines a guardrail for what actions the principals can perform. If you consider this option, since AWS Organizations is not mentioned in this question, so we can't apply an SCP.

36.When a Kinesis data stream is configured as the source of a Firehose delivery stream, Firehose’s PutRecord and PutRecordBatch operations are disabled and Kinesis Agent cannot write to Firehose delivery stream directly. Data needs to be added to the Kinesis data stream through the Kinesis Data Streams PutRecord and PutRecords operations instead.

Kinesis Firehose is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration.


37.You can share the AWS Key Management Service (AWS KMS) customer master key (CMK) that was used to encrypt the snapshot with any accounts that you want to be able to access the snapshot. You can share AWS KMS CMKs with another AWS account by adding the other account to the AWS KMS key policy.

Making an encrypted snapshot of the database will give the auditor a copy of the database, as required for the given use case.


38.RDS stores the DB snapshots in the Amazon S3 bucket belonging to the same AWS region where the RDS instance is located. RDS stores these on your behalf and you do not have direct access to these snapshots in S3, so it's not possible to grant access to the snapshot objects in S3.


39.To migrate accounts from one organization to another, you must have root or IAM access to both the member and master accounts. Here are the steps to follow: 1. Remove the member account from the old organization 2. Send an invite to the member account from the new Organization 3. Accept the invite to the new organization from the member account



40.we can only have as many consumers as shards in Kinesis 


41.When you hibernate an instance, AWS signals the operating system to perform hibernation (suspend-to-disk). Hibernation saves the contents from the instance memory (RAM) to your Amazon EBS root volume. AWS then persists the instance's Amazon EBS root volume and any attached Amazon EBS data volumes.

When you start your instance:
The Amazon EBS root volume is restored to its previous state
The RAM contents are reloaded
The processes that were previously running on the instance are resumed
Previously attached data volumes are reattached and the instance retains its instance ID



.EC2 user data won't help us because it's just here to help us execute a list of commands, not speed them up.
Creating an AMI may help with all the system dependencies, but it won't help us with speeding up the application start time.


42.
