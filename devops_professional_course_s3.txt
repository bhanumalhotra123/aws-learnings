AWS Lambda Versions

When you work on a Lambda function we work on $LATEST
When we are happy with our code state we can publish it can create a new version.
now the version you create say V1 is immutable.
then next version V2 will also be immutable

Versions get their own ARN(Amazon Resource Name)
Version = code + configuration(nothing can be changed)
Each version of the lambda can be accessed


Lambda Aliases

Pointers to Lambda function versions
We can defina a dev, test, prod aliases and have them point at different lambda versions
Aliases are mutable

Aliases enable canary deployment by assigning weights to lambda functions.
Aliases enable stable configuration of our event triggers/ destinations.

Aliases have their own ARNs
Aliases can't refer to other aliases.



Go to Lambda > Create function >  Author from scratch > Function Name > Runtime > Create Function


import json

def lambda_handler(event, context):
    return "this is version1"

>Deploy it 

Then >Test
Create new test event
Event template
Event name
>Create

Then >Test

Say we are very happy with this code and we want name it version1
>Action >Publish new version from $LATEST

>Publish

This has created version1  [Now we can't change the code in it]
Test the function

Now say we want to keep on editing our version

Go back to our main function

import json

def lambda_handler(event, context):
    return "this is version2"

>Deploy 
>Test

>Actions > Publish new version from $LATEST
>Publish


Now we need to play with aliases

Under Versions Tab we can see all the versions we created

Now go to alias tab 
>Create Alias 
Name: dev
Version: $LATEST

>Create Alias
Name: Test
Version: 2

>Create Alias
Name: Prod
Version: 1


With aliases we can start doing some weights

Aliases Tab
Select prod >Edit
>weighted alias >Additional version: 2 Weight: 50
>Save

Now >Test

in results it will show version1 and version2 50% of times each


Once you see version2 is working great 
make weight of version2 100%










Lambda Environment Variables:

Environment variable = key/value pair in "String" form
Adjust the function behaviour without updating code
The environment variables are available to your code
Lambda service adds its own system variables as well


Helpful to store secrets(encrypted by KMS)
Secrets can be encrypted by the Lambda Service key, or your own CMK.

Create a function > Author from Scratch


import json
import os

def lambda_handler(event, context):
    return os.getenv("ENVIRONMENT_NAME")


>Deploy to save the changes

Select this lambda function go to configuration tab
on left hand side > Environment variables

Key
ENVIRONMENT_NAME

Value
dev

There is encryption configuration which we can have but we will do this in security section of course
right now this env variable is unencrypted

>Save

>Test 
Event name = sample event
Create

>Test
In response you should get dev


Again go to environment variables change the value of ENVIRONMENT_NAME to prod and save the function

>Test
response will be 










Lambda Concurrency and Throttling:

Concurrency limit: up to 1000 concurrent executions

Can set a "reserved concurrency" at the function level (=limit)
Each invocation over the concurrency limit will trigger a "Throttle"
Throttle behaviour:
  If synchronous invocation => return ThrottleError -429
  If asynchronous invocation => retry automatically and then go to DLQ
If you need a higher limit, open a support ticket

Lambda Concurrency Issue

If you don't reserve(=limit) concurrency, the following can happen:

users ---> ALB ---> Lambda fn ----> assume it goes upto 1000 concurrent executions

Few users ---> API Gateway ---> Lambda fn     THROTTLE!

SDK/CLI ---> Lambda fn     THROTTLE!




Concurrency and Asynchronous Invocations
            ----New file event----> Lambda invoked
S3 bucket   ----New file event----> Lambda invoked
            ----New file event----> Lambda invoked


If the function doesn't have enough concurrency available to process all events, additional requests are throttled.

For throttling errors(429) and system errors(500-series), Lambda returns the event to the queue and attempts to run the function again for upto 6 hours.

The retry interval increases exponentially from 1 second after first attempt to a maximum of 5 minutes.





Cold Starts and Provisioned Concurrency.

Cold Start: 
   New instance => code is loaded and code outside the handler run (init)
   If the init is large (code, dependencies, SDK..) this process can take some time.
   First request served by the new instances has higher latency than the rest

Provisioned Concurrency:
  Concurrency is allocated before the function is invoked(in advance)
  So the cold start never happens and all invocations have low latency
  Application Auto Scaling can manage concurrency (schedule or target utilization)

Note: 
  Note: cold starts in VPC have been dramatically reduced in Oct & Nov 2019



>Concurrency tab
By default it will show Unreserved account concurrency of 1000 

Unreserved account concurrency is now lower
AWS raises quotas automatically based on usage, or you can request a quota increase.

This default value is shared by all the lambda functions in your account.

>Edit

Reserve concurrency = 20
> save

now you will have 20 for this function and 980 left for others.


To test concurrecny set Reserve concurrency = 0
As this will make the lambda function always throttle


Go to >Test tab hit >Test

Calling the invoke API action failed with this message: Rate Exceeded.

Now change the Reserve concurrency to some bigger value than 0 OR change it to unreserved concurrency.

Test again, it will succeed


We can also provisioned concurrency to remove these cold starts.

>Add

Qualifier type: 
Alias or Version

we can't select latest version here. We want a published version.





Lambda - File Systems Mounting

Lambda functions can access EFS file systems if they are running in a VPC

Configure Lambda to mount EFS file systems to local directory during initialization.

Must leverage EFS Access Points

Limitation: watch out for the EFS connection limits (one function instance = one connection) and connection burst limits

storage options for Lambda
                           max                                Persistence   Content    storageType      sharingPerm     relative data access speed from lambda
Ephermal storage /tmp     10gb                                 Ephermal       dynamic    file system        func only       fastest                            any file system operation
Lambda layers              5layers per fn[total=250mb]         Durable        static     archive            IAM             fastest                             immutable
amazon s3                 Elastic                              Durable        dynamic     object            IAM              fast                              atomic with versioning
amazon efs                Elastic                              Durable        dynamic     file sytem        IAM              very fast                          any file system operation




Lambda - Cross-account efs mounting

Account A                          Account B
VPC A                              VPC B
Lambda Function                    EFS File System + Access Point


1st we have to setup VPC peering


Account A needs:

Lambda Permissions elasticfilesystem:DescribeFileSystems
                   elasticfilesystem:DescribeMountTargets
                   elasticfilesystem:CreateAccessPoint
                   ec2: DescribeSubnets
                   ec2: DescribeNetworkInterfaces




AccountB needs:
EFS File System Policy:

{
"Statement":[
    {
      "Effect": "Allow",
      "Action": [
            "elasticfilesystem:ClientMount"
            "elasticfilesystem:ClientWrite"
      ],
      "Principal": {
           "AWS": "arn:aws:iam::111122223333:root"     ----> accountA
      }



    }
  ]
}





Client --IAM-> Lambda ---CRUD--->DynamoDB 

Client --ALB---> Lambda ---CRUD--->DynamoDB 



API Gateway: Provides us alot of features
 
Client --REST API--> API Gateway   ------Proxy Request----> Lambda ---CRUD--->DynamoDB 



AWS API Gateway
AWS Lambda + API Gateway: No Infra to manage
Support for WebSocket Protocol
Handle API versioning(v1, v2...)
Handle security (Authentication and Authorization)
Create API keys, handle request throttling
Transform and validate requests and responses
Cache API responses.

 

API Gateway Integrations level

Lambda function
  Invoke Lambda function
  Easy way to expose REST API backed by AWS Lambda
HTTP  
  expose HTTP endpoints in the backend 
  example: internal HTTP API on premise , Application Load Balancer
  WHY? Add rate limiting, caching, user authentication, API keys, etc..
AWS Service:
  Expose any AWS API through the API Gateway?
  Example: start an AWS Step Function workflow, post a message to SQS
  Why? Add authentication, deploy publicaly, rate control.

 

API Gateway - AWS service Integration

Kinesis Data Streams example:

Client ---request-->     API Gateway    --send--> Kinesis Data streams---records---> Kinesis Data Firehose  --store .json files--> Amazon S3



3 ways to expose API gateway

Endpoint Types:
Edge-Optimized(default): For global clients
  Requests are routed through the CloudFront Edge locations (improve latency)
  The API Gateway still lives in only one region
Regional:
  For clients within the same region
  Cloud manually combine with CloudFront(more control over the caching strategies and the distribution)
Private:
  Can only be accessed from your VPC using an interface VPC endpoint(ENI)
  Use a resource policy to define access


AWS Gateway- Security

User Authentication through
   IAM Roles(useful for internal applications)
   Cognito (identity for external users - example mobile )
   Custom Authorizer(your own logic)


Custom Domain Name HTTPS security through integration with AWS Certificate Manager(ACM)
  If using Edge-Optimized endpoint, then the certificate must be in us-east-1
  If you are using Regional  endpoint, the certificate must be in the API Gateway region.
  Must setup CNAME or A-alias record in Route53


Hands-ON!


Choose an API type

HTTP API
WebSocket API
REST API -----------------------> Build  this for now
REST API Private



New API  
Clone existing API   
Import API   
Example API


API Name

API Endpoint type
Regional, edge-optimized, private      Picking Regional for now.


>Create API

>Create Method 
DELETE,GET,OPTIONS,HEAD,PATCH,POST,PUT   Select GET for now

> IntegrationType
Lambda Function     ---------> Select this for now       
HTTP
Mock
AWS service
VPC link

But first we need to creat a lambda function
 Author from scratch  ---> Function name = api-gateway-root-get
 Python 3.11 
 Create Function



import json

def lambda_handler(event, context):
    body = "Hello from Lambda!"
    statusCode = 200
    return {
        "statusCode": statusCode,
        "body": json.dumps(body),
        "headers": {
            "Content-Type": "application/json"
        }
    }
  


Deploy

Create a Test Event

>Test  

Now we are going to integrate lambda with api-gateway.

Copt the function ARN


Now go to api-gateway. Once you have selected Lambda function as integrationType
enter the ARN of function

Default timeout is 29sec  [API gateway has limited timeout] doesn't matter how long your lambda functions take to execute.
You can enter custom timeout but it 


>Create Method

This will automaticaly grant API gateway the right to execute lambda function


Refresh lambda function page

It will show that now API gateway is added to lambda function as a trigger.
 

Go to permissions in configuration tab

Go to Resource-based policy statements

Select the statement id and view policy

this will show that your api gateway have permission to invoke the lambda function.



You can go to test tab in aws-api-gateway and test it 



import json

def lambda_handler(event, context):
    print(event)
    body = "Hello from Lambda!"
    statusCode = 200
    return {
        "statusCode": statusCode,
        "body": json.dumps(body),
        "headers": {
            "Content-Type": "application/json"
        }
    }

Now with the printing of event will help you catch cloudwatch logs to see what api-gateway is passing on to lambda functions.

>Test again 

and now go to cloudwatch logs and you will find that api-gateway sends a lot of info back to lambda function.


Create another resource in api-gateway
/houses
select get method
integratelambda function

copy the same code as before just changing the body = hello from houses
click on tests >test

we want this to be tested also from web-browser

>Deploy API

Once deployed you get the invoke url

you can paste it in your browser
 and if you go to /houses you get the expected respose.







API Gateway - Deployment Stages

Making changes in API Gateway does not mean they're effective.
You need to make a "deployment" for them to be in effect.
It is a common source of confusion.

Changes are deployed to "Stages" (as many as you want)
Use the naming you like for stages (dev, test, prod)
Each stage has its own configuration parameters
Stages can be rolled back as a history of deployments is kept.


V1 Client ---https://api.example.com/v1----> API Gateway---v1----> Lambda v1

V2 Client----https://api.example.com/v2----> API Gateway---v2----> Lambda v2


Once your entire client base moves to the new url we can shutdown v1 stage




Stage variables are like environment variables for API Gateway
Use them to change often changing configurations values
They can be used in:
    Lambda function ARN
    HTTP Endpoint
    Parameter mapping templates

Use cases:
   - Configure HTTP endpoints your stages talk to (dev, test, prod...)
   - Pass configuration parameters to AWS Lambda through mapping templates

Stage variables are passed to the "context" object in AWS Lambda 

Format: ${stageVariables.variableName}



API Gateway Stage Variables & Lambda Aliases

We create a stage variable to indicate the corresponding Lambda alias

Our API gateway will automatically invoke the right Lambda function

API Gateway(Dev Stage) ----> Dev Alias ----> $Latest lambda function
API Gateway(Test Stage)----> Test Alias----> v2 lambda function
API Gateway(Prod Stage)----> Prod Alias----> v1 lambda function(95%) and v2 lambda function(5%)



Hands on!
API gateway
Create a resource
> give name as stage-variables

Create a lambda function for version1 >test it >actions> publish it with name v1

again publish another lambda function version2 by changing the main function $LATEST with name v2

now deploy the $LATEST as hello from dev, no need to publish it as it is dev



Now go to alias tab of your lambda function 
NAME DEV
VERSION $LATEST

NAME TEST
VERSION 2

NAME PROD 
VERSION 1 95%
VERSION 2 5%



API Gateway 
> Create method 

MethodType GET
IntegrationType  Lambda fn

Lambda Function
eu-west-1        ARN:$(stageVariables.lambdaAlias)

Copy the permissions that you are prompted it

open cloud shell 
run that
probably ARN is copied twice correct it

remove ${stageVariables.lambdaAlias}   add DEV in place of it

run it on cloud shell


again change it to TEST and again run

last change it to PROD and again run



Now  > Create method

Go to lambda function > Alias > PROD > Resource-based-Policy [you can check the policy it states api gateway can invoke it]



Now in API  Gateway 

> Test

You will find option to set lambdaAlias
Try for TEST once

you will get hello from lambda v2




Deploy API 
Stage: Dev

Once done

Go down you can set stage variables, add lambdaAlias DEV


Deploy API again
Stage name: test

in test stage add stage variables, lambdaAlias TEST

same for prod


you can test by going to different urls

 
Create API Gateway resources and Lambda functions (v1 and v2).
Publish Lambda functions with aliases (Dev, Test, Prod).
Use AWS CloudShell or AWS CLI to dynamically set stage variables for each deployment stage, like this:
For Dev: Set lambdaAlias to 'DEV'
For Test: Set lambdaAlias to 'TEST'
For Prod: Set lambdaAlias to 'PROD'
Test with different Lambda aliases (Dev, Test, Prod).
Configure Resource-Based Policies in Lambda for API Gateway.
Deploy your API with stage variables (Dev, Test, Prod).
Set stage variables like lambdaAlias DEV for dev stage
and lambdaAlias PROD for prod stage







API Gateway - Open API spec

Common way of defining REST APIs, using API definition as code
Import existing OpenAPI 3.0 spec to API Gateway

Method
Method Request
Integration Request
Method Response
+ AWS extensions for API gateway and setup every single option

Can export current API as OpenAPI spec
OpenAPI specs can be written in YAML or JSON



Request API - Request Validation

You can configure API Gateway to perform basic validation of an API request before proceeding with integration request.
When the validation fails, API Gateway immediately fails the request
  Returns a 400 error response to the caller

This reduces unnecessary calls to the backend

Checks:
   The required request parameters in the URI, query string, and headers of an incoming request are included and non-blank. 
   The applicable request payload adheres to the configured JSON schema request model of the method.


REST API -- RequestValidation - OpenAPI

Defines the Validators

{
 "openapi": "3.0.0",
 "info": {
    "title": "ReqValidation Sample",
    "version": "1.0.0"
 },
 "servers: [...],
 "x-amazon-apigateway-request-validators":{
      "all": {
          "validateRequestBody": true,
          "validateRequestParameters": true
      },
      "params-only":{
          "validateRequestBody": false,
          "validateRequestParameters": true
      }
 }

}




Enable params-only Validator on all API methods


{
 "openapi": "3.0.0",
 "info": {
    "title": "ReqValidation Sample",
    "version": "1.0.0"
 },
 "servers: [...],
 "x-amazon-apigateway-request-validators": "params-only",
}






Enable all Validator on the POST/validation method (overrides the params-pnly validator inherited from the API)


{
 "openapi": "3.0.0",
 "info": {
    "title": "ReqValidation Sample",
    "version": "1.0.0"
 },
 "servers: [...],
"paths": {
  "/validation": {
      "post": {
 "x-amazon-apigateway-request-validators": "all",
   }
  }
 },
}





Lets see how to import an OpenAPI definition

Create API > REST API > Import API > Import API  (using this you can upload a file which is in OpenAPI format)



Create API > REST API > Example API> CreateAPI
here it defines how a api should be created


How can we export our current API as an open API?
Go to any stage:
Stage Action > Export API > Pick API specification type as swagger or Open API 3, also pick fomat as JSON or YAML, 

Extensions
Export without extensions
Export with API Gateway extensions
Export with Postman extensions.




You can >Generate SDK

Pick platform: Androidm JavaScript, iOS(Objective-C), iOS(Swift), Java SDK, Ruby

Your application can very easily interact with the SDK created





Caching API responses
Caching reduces the number of calls made to the backend

Client ----> API Gateway ----checks cache----If cache missing ---> backend


Default TTL(time to live ) is 300seconds (min: 0s, max:3600s)
Caches are defined per stage
Possible to override cache settings per method
Cache encryption option
Cache capacity between 0.5GB to 237GB
Cache is expensive, makes sense in production, may not make sense in dev/test



AWS Gateway Cache Invalidation

Able to flush the entire cache immediately

Clients can invalidate the cache with header: Cache-Control:max-age=0(with proper IAM authorization)


{
"Version": "2012-10-17"
"Statement": [
{
  "Effect": "Allow",
  "Action": [
    "execute-api:InvalidateCache"

  ],
  "Resource": [
      "Resource": [
         "arn: api-id/stage-name/GET/resource-path-specifier"
      ]
  ]
}
]
}


If you don't impose an InvalidateCache policy (or choose the Require authorization check box in the console), any client can invalidate the API cache


Go to prod stage
Stage Details >edit
API Cache [switch on]
Choose the cache capacity example - 0.5GB
Cache time to live say 300sec

Per-key cache invalidation > require authorization [switch ON]

Unauthorized request handling >ignore cache control header OR ignore cache control header, add a warning in response header OR Fail the request with 403 status code

Done. Now cache is ON!


You can edit method overrides 
Select one method say GET
say change the TTL and other rules


API Gateway - Canary Deployment

Possibility to enable canary deployments for any stage (usually prod)
Choose the % of traffic the canary channel receives

Prod Stage -------------> Lambda v1
Prod stage canary ---------> Lambda v2

Metrics& Logs are seperate(for better monitoring)
Possibility to override stage variables for canary
this is blue/green deployment with AWS Lambda & API Gateway
























DynamoDB Accelerator

- Fully-managed, highly available, seamless in-memory cache for DynamoDB
- Help solve read congestion by caching
- Microseconds latency for cached data
- Doesn't require application logic modification (compatible with existing DynamoDB APIs)
- 5 minutes TTL for cache (default)



DAX   VS   Amazon ElastiCache


DAX
Individual objects cache
Query & scan cache 


Amazon ElastiCache
Store Aggregaation Result


DynamoDB - Stream Processing
Ordered stream of item-level modifications (create/update/delete) in a table
Use cases:
  React to changes in real-time (welcome email to users)
  Real-time usage analytics



DynamoDB - Streams
24 hours retention
Limited # of consumers 
Process using AWS Lambda Triggers,  


Kinesis Data Streams(newer)
1 year retention
High # of consumers
Process using AWS Lambda, Kinesis Data Analytics, Kinesis Firehost, AWS Glue, Streaming ETL...



DynamoDB Streams
                                                                                                                                  ---filtering/transforming----->DDB Table
Applycation ----create/update/delete ---> Table ---> DynamoDB Streams------>[Processing Layer: DynamoDB KCL Adapter OR Lambda fn] ---messaging, notifications---> Amazon SNS
                                                ----> Kinesis Data Streams ---> Kinesis Data Fire hose --analytics--> Amazon Redshift
                                                                                                       --archiving--> S3
                                                                                                       --indexing---> AWS Opensearch

Many more options

DynamoDB Global Tables

two way replication
Make a DynamoDB table accessible with low latency in multiple-regions
Active-Active replication
Applications can READ and WRITE to the table in any region
Must enable DynamoDB Streams as a pre-requisite

DynamoDB - Time To Live (TTL)
Automatically delete items after an expiry timestamp

Use cases: reduce stored data by keeping only current items, adhere to regulatory obligations, web session handling...


DynamoDB - Backups for disaster recovery

Continuous backups using point-in-time recovery
  Optionally enbaled for the last 35 days
  Point-in-time recovery to any time within the backup window
  The recovery process creates a new table

On-demand backups
 Full backups for long-term retention, until explicitly deleted
 Doesn't affect performance or latency
 Can be configured and managed in AWS Backup (enables cross-region copy)
 The recovery process creates a new table




DynamoDB - Integration with Amazon S3

Export to S3                                    DynamoDB --export-> S3 --Query--> Athena


Export to S3 (must enable PITR)
  Works for any point of time in the last 35 days
  Doesn't affect the read capacity of your table
  Perform data analysis on top of DynamoDB
  Retain snapshots for auditing
  ETL on top of S3 data before importing back into DynamoDB
  Export in DynamoDB JSOn or ION format

Import from S3
  Import CSV, DynamoDB JSON or ION format
  Doesn't consume any write capacity
  Creates a New table 
  Import errors are logged in CloudWatch logs






DMS - Database Migration Service

quickly and securely migrate databases to AWS, resilient, self healing
The source databases remains available during the migration

supports: 
  Homogeneous migrations: ex Oracle to Oracle
  Heterogeneous migrations: ex Microsoft SQL

Continuous Data Replication using CDC
You must create an EC2 instance to perform the replication tasks



SOURCE DB --------> EC2 running DMS software ----> Target Database

AWS Schema Conversion Tool
Convert your Database's Schema from one engine to another


Multi AZ Deployment
