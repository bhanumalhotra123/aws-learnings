Test1a

1.Remember that the more distinct partition key values your workload accesses, the more those requests will be spread across the partitioned space. Conversely, the less distinct partition key values, the less evenly spread it would be across the partitioned space, which effectively slows the performance.


Partition key design is important for DynamoDB because it determines how I/O requests are distributed across the underlying physical partitions.
If you don't design your partition key carefully, you can create "hot" partitions that receive a lot of I/O requests. This can lead to throttling and inefficient use of your provisioned I/O capacity.
To achieve optimal performance, you should aim to spread your I/O requests across as many distinct partition key values as possible. This will help you to use your provisioned I/O capacity more efficiently and avoid throttling.

Here are some tips for designing a good partition key:

Choose a partition key that is unique for each item in your table.
Choose a partition key that is accessed frequently and evenly across your workload.
Avoid using partition keys that have a small number of distinct values.
If you have a large number of items in your table, consider using a composite partition key.

Here are some examples of good partition key designs:
For a table that stores user data: The partition key could be the user ID or the user email address.
For a table that stores product data: The partition key could be the product ID or the product category.
For a table that stores order data: The partition key could be the order ID or the customer ID.

A composite partition key is a partition key that consists of two or more attributes. DynamoDB uses the values of all of the attributes in the composite partition key to determine the partition in which an item is stored.


To create a partition key in DynamoDB, you need to specify it when you create the table. You can do this using the AWS Management Console, the AWS CLI, or the AWS SDKs.

For example, to create a table with a partition key called "UserID" using the AWS Management Console, you would follow these steps:

Go to the Amazon DynamoDB console.
Click Create table.
Enter a name for your table and select the Create checkbox.
In the Partition key section, select Add attribute.
Enter the name of your partition key attribute (e.g., "UserID") and select the data type.
Click Create table.
Once you have created the table, you cannot change the partition key. However, you can add new partition keys to the table by creating a composite partition key.

2.Amazon Aurora supports the following types of endpoints:

Cluster endpoint: This endpoint connects you to the primary instance and automatically follows the primary instance in case of failover, that is, when the current primary instance is demoted and one of the Aurora Replicas is promoted in its place.

Reader endpoint: This endpoint includes all Aurora Replicas in the DB cluster under a single DNS CNAME. You can use the reader endpoint to implement DNS-based random assignment of instance IP addresses to new read-only connections. This can improve read scalability and high availability for your query-intensive applications.

Instance endpoint: Each instance in the DB cluster has its own individual endpoint. You can use instance endpoints to connect to specific instances in your DB cluster, such as for maintenance or troubleshooting.

For example, the primary instance handles all data definition language (DDL) and data manipulation language (DML) statements. Up to 15 Aurora Replicas handle read-only query traffic.

Although it is true that a reader endpoint enables your Aurora database to automatically perform load-balancing among all the Aurora Replicas, it is quite limited to doing read operations only. You still need to use a custom endpoint to load-balance the database connections based on the specified criteria.


A custom endpoint is a type of endpoint that allows you to connect to a subset of the Aurora instances in your DB cluster. You can use custom endpoints to distribute traffic across different groups of Aurora instances, or to isolate certain types of traffic from other types of traffic.
For example, you could create a custom endpoint for your read-only traffic and a custom endpoint for your write-only traffic. This would allow you to scale your read traffic independently of your write traffic.

3.When you create a CMK in KMS, you can choose to have AWS manage the key material for you, or you can import your own key material. If you choose to have AWS manage the key material, AWS will generate and store the key material in the AWS KMS default key store, which is protected by FIPS 140-2 validated cryptographic modules.
If you choose to import your own key material, you must generate the key material in a secure environment and then import it into KMS using the ImportKeyMaterial operation.

AWS managed customer managed keys (CMKs) are not saved in Cloud HSM by default. AWS managed CMKs are generated and stored in the AWS KMS default key store, which is protected by FIPS 140-2 validated cryptographic modules.
You can only store customer managed CMKs in Cloud HSM by creating a custom key store in KMS that is backed by Cloud HSM. Then, when you create a CMK in KMS, you can specify the custom key store. KMS will then generate the key material for the CMK in Cloud HSM and back it up with Cloud HSM.

The AWS Key Management Service (KMS) custom key store feature combines the controls provided by AWS CloudHSM with the integration and ease of use of AWS KMS. You can configure your own CloudHSM cluster and authorize AWS KMS to use it as a dedicated key store for your keys rather than the default AWS KMS key store. When you create keys in AWS KMS you can choose to generate the key material in your CloudHSM cluster. CMKs that are generated in your custom key store never leave the HSMs in the CloudHSM cluster in plaintext and all AWS KMS operations that use those keys are only performed in your HSMs.
Cloud HSM is a hardware security module (HSM) that provides a secure way to store and manage your cryptographic keys.

4.API Gateway will automatically scale and handle massive traffic spikes so you do not have to do anything is incorrect. Although it can scale using AWS Edge locations, you still need to configure the throttling to further manage the bursts of your APIs.
Amazon API Gateway provides throttling at multiple levels including global and by a service call. Throttling limits can be set for standard rates and bursts. For example, API owners can set a rate limit of 1,000 requests per second for a specific method in their REST APIs, and also configure Amazon API Gateway to handle a burst of 2,000 requests per second for a few seconds.

5.AWS WAF is a web application firewall that lets you monitor the HTTP(S) requests that are forwarded to an Amazon CloudFront distribution, an Amazon API Gateway REST API, an Application Load Balancer, or an AWS AppSync GraphQL API.

-Web ACLs – You use a web access control list (ACL) to protect a set of AWS resources. You create a web ACL and define its protection strategy by adding rules. Rules define criteria for inspecting web requests and specify how to handle requests that match the criteria. You set a default action for the web ACL that indicates whether to block or allow through those requests that pass the rules inspections.

-Rules – Each rule contains a statement that defines the inspection criteria and an action to take if a web request meets the criteria. When a web request meets the criteria, that's a match. You can configure rules to block matching requests, allow them through, count them, or run CAPTCHA controls against them.

-Rules groups – You can use rules individually or in reusable rule groups. AWS Managed Rules and AWS Marketplace sellers provide managed rule groups for your use. You can also define your own rule groups.

AWSManagedRulesSQLiRuleSet - The SQL database rule group contains rules to block request patterns associated with the exploitation of SQL databases, like SQL injection attacks. This can help prevent remote injection of unauthorized queries. Evaluate this rule group for use if your application interfaces with an SQL database.

There is no additional software to deploy, DNS configuration, SSL/TLS certificate to manage, or need for a reverse proxy setup.
With AWS Firewall Manager integration, you can centrally define and manage your rules and reuse them across all the web applications that you need to protect.

6.AWS Network Firewall is a managed service that is primarily used to deploy essential network protections for all of your Amazon Virtual Private Clouds (VPCs) and not particularly to your Application Load Balancers. Take note that the AWS Network Firewall is account-specific by default and needs to be integrated with the AWS Firewall Manager to easily share the firewall across your other AWS accounts. In addition, refactoring the web application will require an immense amount of time.

7.An S3 access endpoint is a type of VPC endpoint that allows you to access Amazon S3 from your VPC without requiring an internet gateway or NAT device for your VPC. This can help to improve security and performance by keeping your traffic within the AWS network.

To create an S3 access endpoint, you must first create a route table and associate it with your VPC. Then, you can create the S3 access endpoint and associate it with your route table. Finally, you must update your route table to send traffic to the S3 access endpoint.

Gateway endpoint: A gateway endpoint is a gateway that you specify in your route table to access Amazon S3 from your VPC over the AWS network. Gateway endpoints are available at no additional cost.
Interface endpoint: An interface endpoint extends the functionality of gateway endpoints by using private IP addresses to route requests to Amazon S3 from within your VPC, on premises, or from a VPC in another AWS Region by using VPC peering or AWS Transit Gateway. Interface endpoints are available for an additional cost.

An S3 access endpoint is a type of gateway endpoint. It is a gateway that you specify in your route table to access Amazon S3 from your VPC over the AWS network. Gateway endpoints are available at no additional cost.

Interface endpoints are a different type of VPC endpoint that uses private IP addresses to route requests to Amazon S3 from within your VPC, on premises, or from a VPC in another AWS Region. Interface endpoints are available for an additional cost.

8.Using Amazon CloudWatch to monitor the CPU Utilization of your database is incorrect. Although you can use this to monitor the CPU Utilization of your database instance, it does not provide the percentage of the CPU bandwidth and total memory consumed by each database process in your RDS instance. Take note that CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance while RDS Enhanced Monitoring gathers its metrics from an agent on the instance.
9.The default termination policy is designed to help ensure that your network architecture spans Availability Zones evenly. With the default termination policy, the behavior of the Auto Scaling group is as follows:

 If there are instances in multiple Availability Zones, choose the Availability Zone with the most instances and at least one instance that is not protected from scale in. If there is more than one Availability Zone with this number of instances, choose the Availability Zone with the instances that use the oldest launch configuration.

 Determine which unprotected instances in the selected Availability Zone use the oldest launch configuration. If there is one such instance, terminate it.

 If there are multiple instances to terminate based on the above criteria, determine which unprotected instances are closest to the next billing hour. (This helps you maximize the use of your EC2 instances and manage your Amazon EC2 usage costs.) If there is one such instance, terminate it.

 If there is more than one unprotected instance closest to the next billing hour, choose one of these instances at random.

10.Amazon DynamoDB is integrated with AWS Lambda so that you can create triggers—pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.

11.To require that users enter a password on a password-protected Redis server, include the parameter --auth-token with the correct password when you create your replication group or cluster and on all subsequent commands to the replication group or cluster.  

12.Amazon Macie generates two categories of findings: policy findings and sensitive data findings. A policy finding is a detailed report of a potential policy violation or issue with the security or privacy of an Amazon S3 bucket. Macie generates these findings as part of its ongoing monitoring activities for your Amazon S3 data. A sensitive data finding is a detailed report of sensitive data in an S3 object. Macie generates these findings when it discovers sensitive data in S3 objects that you configure a sensitive data discovery job to analyze.

13.Amazon Polly is simply a service that turns text into lifelike speech, allowing you to create applications that talk, and build entirely new categories of speech-enabled products. Polly can't be used to scane usage patterns on your S3 data.

14. Amazon Kendra is just an enterprise search service that allows developers to add search capabilities to their applications. This enables their end users to discover information stored within the vast amount of content spread across their company

15. Amazon Fraud Detector is only a fully managed service for identifying potentially fraudulent activities and for catching more online fraud faster.

16.By using AWS Secret Manager with a new AWS KMS key, you can add an extra layer of security to your EKS cluster's etcd key-value store. To do this, you need to create a new KMS key in the AWS KMS console, then create a new secret in the AWS Secret Manager console, specifying the new KMS key as the encryption key. Finally, you can configure your EKS cluster to use the new secret by creating a Kubernetes secret object that references the AWS Secret Manager secret.

17.Refactor the application to .NET Core and run it as a serverless container service using Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate is incorrect. This will take significant changes to the application as you will refactor, or do a code change to, the codebase in order for it to become a serverless container application. Remember that the scenario explicitly mentioned that the migration process should minimize development changes. A better solution is to rehost the on-premises .NET application to an AWS Elastic Beanstalk Multi-AZ environment, which doesn't require any code changes.

18.AWS Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring. Elastic Beanstalk supports applications developed in Go, Java, .NET, Node.js, PHP, Python, and Ruby. When you deploy your application, Elastic Beanstalk builds the selected supported platform version and provisions one or more AWS resources, such as Amazon EC2 instances, to run your application.

19.you can assign an IAM Role to the users or groups from your Active Directory once it is integrated with your VPC via the AWS Directory Service AD Connector.

20.Amazon FSx For Windows File Server does not have a parallel file system, unlike Lustre.

21.If you want to serve private content through CloudFront and you're trying to decide whether to use signed URLs or signed cookies, consider the following:

Use signed URLs for the following cases:
- You want to use an RTMP distribution. Signed cookies aren't supported for RTMP distributions.
- You want to restrict access to individual files, for example, an installation download for your application.
- Your users are using a client (for example, a custom HTTP client) that doesn't support cookies.
Use signed cookies for the following cases:
- You want to provide access to multiple restricted files, for example, all of the files for a video in HLS format or all of the files in the subscribers' area of a website.
- You don't want to change your current URLs.

22.This is how client-side encryption using client-side master key works:
When uploading an object - You provide a client-side master key to the Amazon S3 encryption client. The client uses the master key only to encrypt the data encryption key that it generates randomly. The process works like this:

1. The Amazon S3 encryption client generates a one-time-use symmetric key (also known as a data encryption key or data key) locally. It uses the data key to encrypt the data of a single Amazon S3 object. The client generates a separate data key for each object.

2. The client encrypts the data encryption key using the master key that you provide. The client uploads the encrypted data key and its material description as part of the object metadata. The client uses the material description to determine which client-side master key to use for decryption.

3. The client uploads the encrypted data to Amazon S3 and saves the encrypted data key as object metadata (x-amz-meta-x-amz-key) in Amazon S3.

When downloading an object - The client downloads the encrypted object from Amazon S3. Using the material description from the object's metadata, the client determines which master key to use to decrypt the data key. The client uses that master key to decrypt the data key and then uses the data key to decrypt the object.

23.Versioning in S3 helps with mistakenly deleted objects by keeping a history of all changes to an object. This includes both updates to the object's content and deletions of the object. When versioning is enabled for a bucket, S3 creates a new version of an object whenever it is updated or deleted.
If you mistakenly delete an object from a versioned bucket, you can restore the object from a previous version. To do this, you can use the AWS Management Console, the AWS CLI, or the AWS SDKs.

24.AWS Central Tower service is primarily used to manage and govern multiple AWS accounts and not just S3 buckets. Using the AWS Lake Formation service is a more suitable choice.
AWS Lake Formation is a service that makes it easy to set up a secure data lake in days. A data lake is a centralized, curated, and secured repository that stores all your data, both in its original form and prepared for analysis. A data lake enables you to break down data silos and combine different types of analytics to gain insights and guide better business decisions.

26.Amazon S3 forms the storage layer for Lake Formation. If you already use S3, you typically begin by registering existing S3 buckets that contain your data. Lake Formation creates new buckets for the data lake and import data into them. AWS always stores this data in your account, and only you have direct access to it.

AWS Lake Formation is integrated with AWS Glue which you can use to create a data catalog that describes available datasets and their appropriate business applications. Lake Formation lets you define policies and control data access with simple “grant and revoke permissions to data” sets at granular levels. You can assign permissions to IAM users, roles, groups, and Active Directory users using federation. You specify permissions on catalog objects (like tables and columns) rather than on buckets and objects.

27.The visibility timeout is the amount of time that a message is hidden from other consumers after it is received. If a consumer does not finish processing a message before the visibility timeout expires, the message will be made visible to other consumers.

This ensures that messages are not lost, even if a consumer fails. However, it also means that messages can be processed multiple times if they are not deleted after they are processed.

28.AWS Artifact is a great resource for compliance-related information. It provides on-demand access to a variety of AWS security and compliance reports, as well as select online agreements. This can be very helpful for organizations that need to demonstrate their compliance with various regulations and standards.
All AWS Accounts have access to AWS Artifact. Root users and IAM users with admin permissions can download all audit artifacts available to their accounts by agreeing to the associated terms and conditions. You will need to grant IAM users with non-admin permissions access to AWS Artifact using IAM permissions. This allows you to grant a user access to AWS Artifact while restricting access to other services and resources within your AWS Account.

29.Lambda@Edge lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points:
- After CloudFront receives a request from a viewer (viewer request)
- Before CloudFront forwards the request to the origin (origin request)
- After CloudFront receives the response from the origin (origin response)
- Before CloudFront forwards the response to the viewer (viewer response)

30.Since the company is using Microsoft Active Directory which implements Security Assertion Markup Language (SAML), you can set up a SAML-Based Federation for API Access to your AWS cloud. In this way, you can easily connect to AWS using the login credentials of your on-premises network.

Microsoft Active Directory (AD) is a directory service that stores information about users, computers, and other resources in a network domain. AD provides a central location for managing user identities, permissions, and other resources.

Security Assertion Markup Language (SAML) is an open standard for exchanging authentication and authorization data between security systems. SAML can be used to implement federated single sign-on (SSO), which allows users to log in to multiple applications using a single set of credentials.

SAML-based federation for API access to AWS allows you to use your Microsoft AD credentials to access AWS APIs. This can be useful for organizations that want to centralize user management and authentication.

To set up SAML-based federation for API access to AWS, you will need to:

Configure your Microsoft AD environment to trust AWS.
Configure AWS to trust your Microsoft AD environment.
Create a SAML identity provider in AWS.
Create IAM roles and policies for users who will use SAML to access your AWS resources.
Test your SAML federation configuration.

30.AWS Transfer for SFTP is a fully managed service that enables you to securely transfer files between your on-premises servers and Amazon S3 using the Secure File Transfer Protocol (SFTP).

Amazon API Gateway provides throttling at multiple levels including global and by service call. Throttling limits can be set for standard rates and bursts. For example, API owners can set a rate limit of 1,000 requests per second for a specific method in their REST APIs, and also configure Amazon API Gateway to handle a burst of 2,000 requests per second for a few seconds. Amazon API Gateway tracks the number of requests per second. Any request over the limit will receive a 429 HTTP response. The client SDKs generated by Amazon API Gateway retry calls automatically when met with this response. Hence, enabling throttling limits and result caching in API Gateway is the correct answer.

You can add caching to API calls by provisioning an Amazon API Gateway cache and specifying its size in gigabytes. The cache is provisioned for a specific stage of your APIs. This improves performance and reduces the traffic sent to your back end. Cache settings allow you to control the way the cache key is built and the time-to-live (TTL) of the data stored for each method. Amazon API Gateway also exposes management APIs that help you invalidate the cache for each stage.

31.The first time you invoke your function, AWS Lambda creates an instance of the function and runs its handler method to process the event. When the function returns a response, it stays active and waits to process additional events. If you invoke the function again while the first event is being processed, Lambda initializes another instance, and the function processes the two events concurrently. As more events come in, Lambda routes them to available instances and creates new instances as needed. When the number of requests decreases, Lambda stops unused instances to free up the scaling capacity for other functions.

Lambda can scale faster than the regular Auto Scaling feature of Amazon EC2, Amazon Elastic Beanstalk, or Amazon ECS. This is because AWS Lambda is more lightweight than other computing services. Under the hood, Lambda can run your code to thousands of available AWS-managed EC2 instances (that could already be running) within seconds to accommodate traffic. This is faster than the Auto Scaling process of launching new EC2 instances that could take a few minutes or so. An alternative is to overprovision your compute capacity but that will incur significant costs 

32.RDS events only provide operational events such as DB instance events, DB parameter group events, DB security group events, and DB snapshot events. What we need in the scenario is to capture data-modifying events (INSERT, DELETE, UPDATE) which can be achieved thru native functions or stored procedures.

33.maximum days for the EFS lifecycle policy is only 90 days. The requirement is to move the files that are older than 2 years or 730 days.

34.Although an EBS Volume can be attached to multiple EC2 instances, you can only do so on instances within an availability zone. What we need is high-available storage that can span multiple availability zones. 

EBS multi-attach is a feature that can be used to attach an EBS volume to multiple EC2 instances in the same Availability Zone. This can be useful for applications that need to share data between multiple servers, such as a database or a file system.

EFS is a shared file system that can be accessed by multiple EC2 instances in different Availability Zones. EFS is a good choice for applications that need to share data between servers in different Availability Zones, such as a website or a content delivery network (CDN).

Here is a table that summarizes the key differences between EBS multi-attach and EFS:

Feature	EBS multi-attach	EFS
Availability	Available in a single Availability Zone	Available across multiple Availability Zones
Performance	High	High
Scalability	Scalable up to terabytes	Scalable up to petabytes
Use cases	Databases, file systems, applications that need to share data between servers in the same Availability Zone	Websites, CDNs, applications that need to share data between servers in different Availability Zones
When to use EBS multi-attach instead of EFS depends on your specific needs and requirements. EBS multi-attach is a good choice if you need to share data between multiple servers in the same Availability Zone and high performance is critical. EFS is a good choice if you need to share data between servers in different Availability Zones and scalability and high availability are more important than performance.

Here are some examples of when to use EBS multi-attach:

A database that needs to be highly available and have low latency.
A file system that needs to be accessed by multiple servers in the same Availability Zone and performance is critical.
An application server that needs to store and serve large amounts of data and performance is critical.
Here are some examples of when to use EFS:

A website that needs to be highly available and scalable.
A CDN that needs to store and serve large amounts of static content.
An application that needs to share data between servers in different Availability Zones, such as a distributed file system or a messaging system.

35.CloudWatch has available Amazon EC2 Metrics for you to use for monitoring. CPU Utilization identifies the processing power required to run an application upon a selected instance. Network Utilization identifies the volume of incoming and outgoing network traffic to a single instance. Disk Reads metric is used to determine the volume of the data the application reads from the hard disk of the instance. This can be used to determine the speed of the application. However, there are certain metrics that are not readily available in CloudWatch such as memory utilization, disk space utilization, and many others which can be collected by setting up a custom metric.

You need to prepare a custom metric using CloudWatch Monitoring Scripts which is written in Perl. You can also install CloudWatch Agent to collect more system-level metrics from Amazon EC2 instances. Here's the list of custom metrics that you can set up:

- Memory utilization
- Disk swap utilization
- Disk space utilization
- Page file utilization
- Log collection

Q 46 -52
